{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8143731,"sourceType":"datasetVersion","datasetId":4815310}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"1. Tokenization Model - Word Tokenizer","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom collections import Counter\n\n# Load dataset\n# Replace 'path_to_dataset.csv' with the actual path to the IMDb dataset on your system.\ndf = pd.read_csv(\"/kaggle/input/imdb-movie-reviews/IMDB Dataset.csv\")\n\n# Inspect the dataset\nprint(df.head())\n\n# Assume the dataset has a column named 'review' with textual data.\n# Preprocessing function\ndef preprocess_text(text):\n    \"\"\"\n    Clean and preprocess text.\n    - Lowercase conversion\n    - Remove special characters and punctuation\n    - Remove extra whitespace\n    \"\"\"\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra whitespace\n    return text\n\n# Apply preprocessing to the dataset\ndf[\"cleaned_review\"] = df[\"review\"].apply(preprocess_text)\n\n# Tokenization function\ndef tokenize_text(text):\n    \"\"\"\n    Split text into tokens (words).\n    \"\"\"\n    return text.split()\n\n# Tokenize the cleaned reviews\ndf[\"tokens\"] = df[\"cleaned_review\"].apply(tokenize_text)\n\n# Build Vocabulary\ndef build_vocabulary(tokenized_texts):\n    \"\"\"\n    Build a vocabulary from tokenized texts.\n    \"\"\"\n    # Flatten the list of tokens\n    all_tokens = [token for tokens in tokenized_texts for token in tokens]\n    # Count frequency of each token\n    token_counts = Counter(all_tokens)\n    # Assign a unique ID to each token (starting from 1, reserve 0 for padding)\n    vocabulary = {token: idx for idx, (token, _) in enumerate(token_counts.items(), start=1)}\n    return vocabulary\n\n# Create vocabulary from all tokenized reviews\nvocabulary = build_vocabulary(df[\"tokens\"])\n\n# Display vocabulary size\nprint(f\"Vocabulary size: {len(vocabulary)}\")\n\n# Token-to-ID mapping function\ndef tokens_to_ids(tokens, vocab):\n    \"\"\"\n    Convert tokens to their corresponding IDs based on the vocabulary.\n    \"\"\"\n    return [vocab[token] for token in tokens if token in vocab]\n\n# Map tokens to IDs\ndf[\"token_ids\"] = df[\"tokens\"].apply(lambda tokens: tokens_to_ids(tokens, vocabulary))\n\n# Inspect the processed data\nprint(df[[\"review\", \"cleaned_review\", \"tokens\", \"token_ids\"]].head())\n\n# Save the tokenizer and vocabulary for reuse\nimport pickle\n\nwith open(\"tokenizer_vocab.pkl\", \"wb\") as f:\n    pickle.dump(vocabulary, f)\n\nprint(\"Tokenizer and vocabulary saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T16:08:12.949481Z","iopub.execute_input":"2025-01-09T16:08:12.949859Z","iopub.status.idle":"2025-01-09T16:08:27.736137Z","shell.execute_reply.started":"2025-01-09T16:08:12.949810Z","shell.execute_reply":"2025-01-09T16:08:27.735079Z"}},"outputs":[{"name":"stdout","text":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive\nVocabulary size: 181066\n                                              review  \\\n0  One of the other reviewers has mentioned that ...   \n1  A wonderful little production. <br /><br />The...   \n2  I thought this was a wonderful way to spend ti...   \n3  Basically there's a family where a little boy ...   \n4  Petter Mattei's \"Love in the Time of Money\" is...   \n\n                                      cleaned_review  \\\n0  one of the other reviewers has mentioned that ...   \n1  a wonderful little production br br the filmin...   \n2  i thought this was a wonderful way to spend ti...   \n3  basically theres a family where a little boy j...   \n4  petter matteis love in the time of money is a ...   \n\n                                              tokens  \\\n0  [one, of, the, other, reviewers, has, mentione...   \n1  [a, wonderful, little, production, br, br, the...   \n2  [i, thought, this, was, a, wonderful, way, to,...   \n3  [basically, theres, a, family, where, a, littl...   \n4  [petter, matteis, love, in, the, time, of, mon...   \n\n                                           token_ids  \n0  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...  \n1  [50, 192, 193, 194, 29, 29, 3, 195, 196, 23, 1...  \n2  [118, 272, 22, 35, 50, 192, 273, 61, 274, 275,...  \n3  [350, 351, 50, 352, 86, 50, 193, 353, 354, 355...  \n4  [400, 401, 402, 44, 3, 275, 2, 403, 23, 50, 40...  \nTokenizer and vocabulary saved!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\n# Convert vocabulary to a DataFrame\nvocab_df = pd.DataFrame(list(vocabulary.items()), columns=[\"Word\", \"ID\"])\n\n# Save to CSV\nvocab_df.to_csv(\"vocabulary.csv\", index=False)\n\nprint(\"Vocabulary saved to 'vocabulary.csv'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T16:10:29.032789Z","iopub.execute_input":"2025-01-09T16:10:29.033186Z","iopub.status.idle":"2025-01-09T16:10:29.384766Z","shell.execute_reply.started":"2025-01-09T16:10:29.033156Z","shell.execute_reply":"2025-01-09T16:10:29.383829Z"}},"outputs":[{"name":"stdout","text":"Vocabulary saved to 'vocabulary.csv'.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"","metadata":{}}]}